Layer 0:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 8092])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([8092, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([8092, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 1:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 4017])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([4017, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([4017, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 2:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 7992])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([7992, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([7992, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 3:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 6294])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([6294, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([6294, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 4:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 8960])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 5:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 5968])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([5968, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([5968, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 6:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 8275])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([8275, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([8275, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 7:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 7403])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([7403, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([7403, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 8:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 7185])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([7185, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([7185, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 9:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 5728])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([5728, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([5728, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 10:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 4295])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([4295, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([4295, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 11:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 3943])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([3943, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([3943, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 12:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 4252])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([4252, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([4252, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 13:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 3200])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([3200, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([3200, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 14:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 3517])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([3517, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([3517, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 15:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2431])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2431, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2431, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 16:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2747])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2747, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2747, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 17:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2561])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2561, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2561, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 18:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2937])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2937, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2937, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 19:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 3061])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([3061, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([3061, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 20:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2210])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2210, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2210, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 21:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2392])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2392, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2392, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 22:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2671])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2671, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2671, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 23:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2458])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2458, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2458, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 24:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2313])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2313, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2313, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 25:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2395])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2395, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2395, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 26:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 3445])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([3445, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([3445, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 27:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 4222])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([4222, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([4222, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
