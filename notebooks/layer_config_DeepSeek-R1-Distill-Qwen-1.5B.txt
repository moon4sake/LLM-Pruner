Layer 0:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 8956])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([8956, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([8956, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 1:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2569])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2569, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2569, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 2:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 7005])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([7005, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([7005, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 3:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 7709])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([7709, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([7709, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 4:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 8960])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 5:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 6964])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([6964, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([6964, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 6:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 8960])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 7:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 8960])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 8:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 8960])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 9:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 8960])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 10:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 4557])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([4557, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([4557, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 11:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 8960])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 12:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 8960])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([8960, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 13:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2389])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2389, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2389, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 14:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2819])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2819, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2819, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 15:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 1680])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([1680, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([1680, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 16:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 1999])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([1999, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([1999, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 17:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 1817])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([1817, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([1817, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 18:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2031])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2031, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2031, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 19:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 1793])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([1793, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([1793, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 20:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 992])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([992, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([992, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 21:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 1152])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([1152, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([1152, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 22:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 1200])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([1200, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([1200, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 23:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 1116])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([1116, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([1116, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 24:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 956])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([956, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([956, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 25:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 950])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([950, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([950, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 26:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 1277])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([1277, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([1277, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
Layer 27:
  Submodule: input_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: mlp, Param: down_proj.weight, shape=torch.Size([1536, 2637])
  Submodule: mlp, Param: gate_proj.weight, shape=torch.Size([2637, 1536])
  Submodule: mlp, Param: up_proj.weight, shape=torch.Size([2637, 1536])
  Submodule: post_attention_layernorm, Param: weight, shape=torch.Size([1536])
  Submodule: self_attn, Param: k_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: k_proj.weight, shape=torch.Size([256, 1536])
  Submodule: self_attn, Param: o_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: q_proj.bias, shape=torch.Size([1536])
  Submodule: self_attn, Param: q_proj.weight, shape=torch.Size([1536, 1536])
  Submodule: self_attn, Param: v_proj.bias, shape=torch.Size([256])
  Submodule: self_attn, Param: v_proj.weight, shape=torch.Size([256, 1536])
